{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Faster R-CNN\n",
        "\n"
      ],
      "metadata": {
        "id": "9RmVBNrlqCRt"
      },
      "id": "9RmVBNrlqCRt"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "43046a72-2d65-45c8-967f-2f2c690b60b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "43046a72-2d65-45c8-967f-2f2c690b60b8",
        "outputId": "9d7e4fcb-b429-4b26-c199-804be8148878"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIt is an object detection model that identifies objects in an image and draws bounding boxes around them, while also classifying what those objects are.\\n\\n\\nHere we do two things simultaneously, that is Classification and Regression. Classification to identify class of Object and Regression to find the\\ncordinates of bounding boxes around it.\\n\\nComponents:\\n\\nBackbone Network : Any pretrained Model - Feature Extractor\\n\\n\\nhttps://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "It is an object detection model that identifies objects in an image and draws bounding boxes around them, while also classifying what those objects are.\n",
        "\n",
        "\n",
        "Here we do two things simultaneously, that is Classification and Regression. Classification to identify class of Object and Regression to find the\n",
        "cordinates of bounding boxes around it.\n",
        "\n",
        "Components:\n",
        "\n",
        "Backbone Network : Any pretrained Model - Feature Extractor\n",
        "\n",
        "\n",
        "https://medium.com/@fractal.ai/guide-to-build-faster-rcnn-in-pytorch-42d47cb0ecd3\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "NqowsqOzOI8u"
      },
      "id": "NqowsqOzOI8u",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_img = torch.zeros((1, 3, 800, 800)).float() # batch size, channels, height, width"
      ],
      "metadata": {
        "id": "tCFy2ZcLPFwU"
      },
      "id": "tCFy2ZcLPFwU",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]]) # [y1, x1, y2, x2] format"
      ],
      "metadata": {
        "id": "tTCGSFxCYw44"
      },
      "id": "tTCGSFxCYw44",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.LongTensor([6,8])"
      ],
      "metadata": {
        "id": "HE_sfjCiYxkZ"
      },
      "id": "HE_sfjCiYxkZ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_sample = 16"
      ],
      "metadata": {
        "id": "w-0IYmKrZIZC"
      },
      "id": "w-0IYmKrZIZC",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all layers of VGG16, which we will be using as Backbone for our RPN"
      ],
      "metadata": {
        "id": "LU2LObqfZI5q"
      },
      "id": "LU2LObqfZI5q",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fCpRxmvcCNw",
        "outputId": "0a98a59b-459b-4c28-f294-2f266810152e"
      },
      "id": "6fCpRxmvcCNw",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The VGG16 model is around 500mb in sizes coz it has total of 138 million parameters, and each parameter\n",
        "is of float32 (4 bytes) size, and 138 million x 4 byte is around 500MBs.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "vfA9CEXOdpHC",
        "outputId": "2b8a4a67-d090-4b70-ed5a-9dc520c07754"
      },
      "id": "vfA9CEXOdpHC",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nThe VGG16 model is around 500mb in sizes coz it has total of 138 million parameters, and each parameter\\nis of float32 (4 bytes) size, and 138 million x 4 byte is around 500MBs.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Basically model contain two types of layers: Feature Layers and Clasifier Layers\n",
        "Here we have to do work only with Feature Layers, which are responsible for Extracting Features from the input image.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Y-cPADS-e43P",
        "outputId": "9d920065-ce84-4867-c23b-d69bff784f2d"
      },
      "id": "Y-cPADS-e43P",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBasically model contain two types of layers: Feature Layers and Clasifier Layers\\nHere we have to do work only with Feature Layers, which are responsible for Extracting Features from the input image.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.features # Contain all CNN layers, excluding all Fully Connected Layers(Classifier Layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvjaB6IffERP",
        "outputId": "660535b3-06eb-4cc7-bfae-68bd80a0f4c0"
      },
      "id": "XvjaB6IffERP",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (18): ReLU(inplace=True)\n",
              "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (25): ReLU(inplace=True)\n",
              "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (27): ReLU(inplace=True)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zndmlj2fMHn",
        "outputId": "c6dd4d8b-a427-47fd-f81b-32b8142a638f"
      },
      "id": "7zndmlj2fMHn",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now we have to configure VGG16 model, that matches our required Input and Output sizes. This is why we\n",
        "have created dummy_img, just to experiment on it.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "wLUtohseh8fs",
        "outputId": "1a30c4f9-9571-45b0-cfc8-df36226034aa"
      },
      "id": "wLUtohseh8fs",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow we have to configure VGG16 model, that matches our required Input and Output sizes. This is why we \\nhave created dummy_img, just to experiment on it.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fe = list(model.features)"
      ],
      "metadata": {
        "id": "a8_ACd5Kigtf"
      },
      "id": "a8_ACd5Kigtf",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHBrM4jGi8Um",
        "outputId": "aee2f12d-73c5-4a74-f824-61eeb9b4b06e"
      },
      "id": "xHBrM4jGi8Um",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "req_features=[]\n",
        "k = dummy_img.clone()\n",
        "out_channels = 0\n",
        "for i in fe:\n",
        "  k = i(k)\n",
        "  if k.size()[2] < 800//16:\n",
        "    break\n",
        "  req_features.append(i)\n",
        "  out_channels = k.size()[1]"
      ],
      "metadata": {
        "id": "UqTTMD80i8uF"
      },
      "id": "UqTTMD80i8uF",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "800//16  # // is integral division or floor diviion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOQ5pEb0jU7s",
        "outputId": "52fc263a-c81f-4ef1-85c6-075a0b45025d"
      },
      "id": "MOQ5pEb0jU7s",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(req_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6dikpXKnAmI",
        "outputId": "da5c235a-8ef7-4e48-d1ab-b70dd8ee76e1"
      },
      "id": "J6dikpXKnAmI",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLggWUk-nDL_",
        "outputId": "533598a2-8ff0-476f-8ee5-adbcec41b194"
      },
      "id": "FLggWUk-nDL_",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "req_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxr6nx4vnWhH",
        "outputId": "dbb7eb96-1019-48ab-fb8b-afa369a3c3e9"
      },
      "id": "bxr6nx4vnWhH",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True),\n",
              " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
              " ReLU(inplace=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faster_rcnn_fe_extractor = torch.nn.Sequential(*req_features)\n",
        "# here * is unpacking operator, this will unpack list for form seprate arguments for nn.Sequential"
      ],
      "metadata": {
        "id": "9phIvXoxncNp"
      },
      "id": "9phIvXoxncNp",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_map = faster_rcnn_fe_extractor(dummy_img)"
      ],
      "metadata": {
        "id": "q9Fu-5ZzoGcJ"
      },
      "id": "q9Fu-5ZzoGcJ",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_map.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-_YZOJ8oOUL",
        "outputId": "bc1c1565-0ef3-4bfd-bab6-52f744b6f3a6"
      },
      "id": "l-_YZOJ8oOUL",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512, 50, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anchor Boxes"
      ],
      "metadata": {
        "id": "W_bTtClXsPaO"
      },
      "id": "W_bTtClXsPaO"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "So our next task is to define the Anchor Boxes, over feature maps.\n",
        "Basically, There are anchor boxes, of diffrent shape and sizes, that we are gonna slide over feature maps.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "eGy335EkoQ_Y",
        "outputId": "1d8af9e4-4b0d-4cb4-a47c-04342582dbf0"
      },
      "id": "eGy335EkoQ_Y",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSo our next task is to define the Anchor Boxes, over feature maps.\\nBasically, There are anchor boxes, of diffrent shape and sizes, that we are gonna slide over feature maps.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "QY7pgYcSMEIW"
      },
      "id": "QY7pgYcSMEIW",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratios = [0.5, 1, 2] # diffrent ratio of anchor baxoes, i.e, define the diffrent shapes : tall, square and wide"
      ],
      "metadata": {
        "id": "65jdTZUZMg8Y"
      },
      "id": "65jdTZUZMg8Y",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_scales = [8, 16, 32] # diffrent size of anchor boxes i.e, small, medium and large"
      ],
      "metadata": {
        "id": "DYJmBHu7NBTV"
      },
      "id": "DYJmBHu7NBTV",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_base = np.zeros((len(ratios)*len(anchor_scales), 4), dtype=np.float32)"
      ],
      "metadata": {
        "id": "52RNhiAyNThK"
      },
      "id": "52RNhiAyNThK",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_base # format : x1, y1, x2, y2"
      ],
      "metadata": {
        "id": "VWk0P5zHNzDk",
        "outputId": "764896c0-a509-4030-8d67-da07d38d3ae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VWk0P5zHNzDk",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "There will be total of 9(3x3) anchor boxes, which will combine all predefined shape and sizes.\n",
        "\n",
        "Key understandings:\n",
        "\n",
        "-> One pixel in the 50×50 feature map corresponds to a region of 16×16 pixels in the original image,\n",
        "as we downsampled our image by the factor of 16.\n",
        "\n",
        "-> Each pixel in the 50×50 feature map \"looks at\" a receptive field of 16×16 pixels in the input image.\n",
        "  So:\n",
        "\n",
        "    Pixel (0, 0) in the feature map represents image area (0:16, 0:16)\n",
        "\n",
        "    Pixel (1, 0) represents (16:32, 0:16)\n",
        "\n",
        "    Pixel (0, 1) represents (0:16, 16:32)\n",
        "\n",
        "    And so on.\n",
        "\n",
        "We use this mapping to place anchor boxes in image coordinates.\n",
        "\n",
        "\n",
        "-> The anchor box center is aligned with the center of the 16×16 region that a feature map pixel corresponds to in the input image.\n",
        "\n",
        "-> For feature map location (0, 0), the center in image space is at (8, 8)\n",
        "For (1, 0), center = (8 + 16, 8) = (24, 8)\n",
        "For (x, y), center = (16 * x + 8, 16 * y + 8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xuQE9XSfN44q",
        "outputId": "a50151c1-1466-4f88-ff66-136e06111240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "id": "xuQE9XSfN44q",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThere will be total of 9(3x3) anchor boxes, which will combine all predefined shape and sizes.\\n\\nKey understandings:\\n\\n-> One pixel in the 50×50 feature map corresponds to a region of 16×16 pixels in the original image,\\nas we downsampled our image by the factor of 16.\\n\\n-> Each pixel in the 50×50 feature map \"looks at\" a receptive field of 16×16 pixels in the input image.\\n  So:\\n\\n    Pixel (0, 0) in the feature map represents image area (0:16, 0:16)\\n\\n    Pixel (1, 0) represents (16:32, 0:16)\\n\\n    Pixel (0, 1) represents (0:16, 16:32)\\n\\n    And so on.\\n\\nWe use this mapping to place anchor boxes in image coordinates.\\n\\n\\n-> The anchor box center is aligned with the center of the 16×16 region that a feature map pixel corresponds to in the input image.\\n\\n-> For feature map location (0, 0), the center in image space is at (8, 8)\\nFor (1, 0), center = (8 + 16, 8) = (24, 8)\\nFor (x, y), center = (16 * x + 8, 16 * y + 8)\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_sample"
      ],
      "metadata": {
        "id": "EO6wY1HpONEs",
        "outputId": "370ea17f-eb92-425d-db07-7766b8aee1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EO6wY1HpONEs",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctr_y = sub_sample/2\n",
        "ctr_x = sub_sample/2"
      ],
      "metadata": {
        "id": "bPODh85Pe-58"
      },
      "id": "bPODh85Pe-58",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ctr_y, ctr_x)"
      ],
      "metadata": {
        "id": "wHrR4cqpfG-4",
        "outputId": "37056723-08e4-4322-c9b7-06e8e19ee7ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wHrR4cqpfG-4",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8.0, 8.0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(ratios)):\n",
        "  for j in range(len(anchor_scales)):\n",
        "    h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "    w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
        "    index = i * len(anchor_scales) + j\n",
        "    anchor_base[index, 0] = ctr_y - h / 2.\n",
        "    anchor_base[index, 1] = ctr_x - w / 2.\n",
        "    anchor_base[index, 2] = ctr_y + h / 2.\n",
        "    anchor_base[index, 3] = ctr_x + w / 2."
      ],
      "metadata": {
        "id": "XOhBLYE4fK6w"
      },
      "id": "XOhBLYE4fK6w",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_base"
      ],
      "metadata": {
        "id": "gZD6xuCclhIG",
        "outputId": "70dc96da-3861-4c07-ffe5-74096905a2a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gZD6xuCclhIG",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
              "       [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
              "       [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
              "       [ -56.      ,  -56.      ,   72.      ,   72.      ],\n",
              "       [-120.      , -120.      ,  136.      ,  136.      ],\n",
              "       [-248.      , -248.      ,  264.      ,  264.      ],\n",
              "       [ -82.50967 ,  -37.254833,   98.50967 ,   53.254833],\n",
              "       [-173.01933 ,  -82.50967 ,  189.01933 ,   98.50967 ],\n",
              "       [-354.03867 , -173.01933 ,  370.03867 ,  189.01933 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now each pixel represent an image space of 16x16 in which anchor boxes are to be drawn.\n",
        "We have total of 9 such boxes in each pixel and total of 50x50 pixels in features maps.\n",
        "So total number of anchor boxes are 9*50*50 which is 22500.\n",
        "\n",
        "\n",
        "RECALL : The anchor is genrated at each pixel of our Feature Map, and each pixel of Feature Map will be treated\n",
        "as center for the anchor. Each pixel of 50x50 feature map, represent the 16x16 area of Original Image.\n",
        "\n",
        "So,\n",
        "\n",
        "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
        "ctr_y = np.arange(16, (fe_size+1) * 16, 16)\n",
        "\n",
        "using this we will find the center point of that 16x16 image space in ouriginal 800x800 image. This is done by:\n",
        "\n",
        "  ctr_x[i] - 8\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4tJRuLJ9mFNF",
        "outputId": "e18fcc7f-d1ce-4327-8a03-6cfdda660245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "id": "4tJRuLJ9mFNF",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow each pixel represent an image space of 16x16 in which anchor boxes are to be drawn.\\nWe have total of 9 such boxes in each pixel and total of 50x50 pixels in features maps.\\nSo total number of anchor boxes are 9*50*50 which is 22500.\\n\\n\\nRECALL : The anchor is genrated at each pixel of our Feature Map, and each pixel of Feature Map will be treated \\nas center for the anchor. Each pixel of 50x50 feature map, represent the 16x16 area of Original Image.\\n\\nSo,\\n\\nctr_x = np.arange(16, (fe_size+1) * 16, 16)\\nctr_y = np.arange(16, (fe_size+1) * 16, 16)\\n\\nusing this we will find the center point of that 16x16 image space in ouriginal 800x800 image. This is done by:\\n\\n  ctr_x[i] - 8\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fe_size = (800//16)"
      ],
      "metadata": {
        "id": "DeLD9tFcizye"
      },
      "id": "DeLD9tFcizye",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
        "ctr_y = np.arange(16, (fe_size+1) * 16, 16)"
      ],
      "metadata": {
        "id": "04XD0fw_jL5a"
      },
      "id": "04XD0fw_jL5a",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctr_x"
      ],
      "metadata": {
        "id": "fbgy9-96jQDi",
        "outputId": "131411f9-1462-4a1a-d192-61c4c286103a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fbgy9-96jQDi",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 16,  32,  48,  64,  80,  96, 112, 128, 144, 160, 176, 192, 208,\n",
              "       224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416,\n",
              "       432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624,\n",
              "       640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctr = np.zeros((len(ctr_x) * len(ctr_y), 2), dtype=np.float32)"
      ],
      "metadata": {
        "id": "LQT4LHk1paWA"
      },
      "id": "LQT4LHk1paWA",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "for x in range(len(ctr_x)):\n",
        "  for y in range(len(ctr_y)):\n",
        "    ctr[index, 1] = ctr_x[x] - 8\n",
        "    ctr[index, 0] = ctr_y[y] - 8\n",
        "    index += 1"
      ],
      "metadata": {
        "id": "37EhNz5HjRxC"
      },
      "id": "37EhNz5HjRxC",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ctr_x)"
      ],
      "metadata": {
        "id": "roDnsoXNpYu4",
        "outputId": "04cfe473-6e37-4bf8-8dc4-6e6547a38e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "roDnsoXNpYu4",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctr"
      ],
      "metadata": {
        "id": "TgopAQdapZPP",
        "outputId": "59378e25-83f9-428f-a4d0-8a13b952cbd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TgopAQdapZPP",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  8.,   8.],\n",
              "       [ 24.,   8.],\n",
              "       [ 40.,   8.],\n",
              "       ...,\n",
              "       [760., 792.],\n",
              "       [776., 792.],\n",
              "       [792., 792.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctr.shape # so in total we have 2500 anchor centers"
      ],
      "metadata": {
        "id": "Ui2H2mg3q6K1",
        "outputId": "c8d360e9-1c48-4f5e-89d2-3dd7de9edc20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ui2H2mg3q6K1",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now at each center, we need to generate the anchor boxes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PPnLMFFcr-al",
        "outputId": "30218bdd-b102-4f0f-d70b-95d89afbfa78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "PPnLMFFcr-al",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow at each center, we need to generate the anchor boxes.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anchors = np.zeros((fe_size*fe_size*9, 4))"
      ],
      "metadata": {
        "id": "AKxPv9phxpiD"
      },
      "id": "AKxPv9phxpiD",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchors"
      ],
      "metadata": {
        "id": "W_6kE-C3xyrX",
        "outputId": "dc4e11ca-6f1d-420c-c7d5-a2281714a4ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W_6kE-C3xyrX",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anchors.shape # total 22500 anchor boxes"
      ],
      "metadata": {
        "id": "v2TlKC-4xzgj",
        "outputId": "cc7c70b8-ad08-41e3-e54d-db05672f24c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "v2TlKC-4xzgj",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22500, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "for c in ctr:\n",
        "  ctr_y, ctr_x = c\n",
        "  for i in range(len(ratios)):\n",
        "    for j in range(len(anchor_scales)):\n",
        "      h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "      w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
        "      anchors[index, 0] = ctr_y - h / 2.\n",
        "      anchors[index, 1] = ctr_x - w / 2.\n",
        "      anchors[index, 2] = ctr_y + h / 2.\n",
        "      anchors[index, 3] = ctr_x + w / 2.\n",
        "      index += 1"
      ],
      "metadata": {
        "id": "vRafkSVCyLqE"
      },
      "id": "vRafkSVCyLqE",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchors"
      ],
      "metadata": {
        "id": "A_Tm9BZRzUdR",
        "outputId": "f6d78a84-487b-4064-ddf0-d3486e72d858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A_Tm9BZRzUdR",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -37.254834  ,  -82.50966799,   53.254834  ,   98.50966799],\n",
              "       [ -82.50966799, -173.01933598,   98.50966799,  189.01933598],\n",
              "       [-173.01933598, -354.03867197,  189.01933598,  370.03867197],\n",
              "       ...,\n",
              "       [ 701.49033201,  746.745166  ,  882.50966799,  837.254834  ],\n",
              "       [ 610.98066402,  701.49033201,  973.01933598,  882.50966799],\n",
              "       [ 429.96132803,  610.98066402, 1154.03867197,  973.01933598]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Let say below given some data, belonging to Ground Truth. Now our next task is to find anchor boxes, that fit\n",
        "them the most. Faster RCNN has specific guidelines to assiging the labels to the anchor boxes.\n",
        "\n",
        "There will be two kind of lables: positive and negative.\n",
        "\n",
        "We will be assiging positive lable to two kind of anchor:\n",
        "  a) The anchor with highest Intersection-over-Union(IoU) with ground truth box.\n",
        "  b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box.\n",
        "\n",
        "\n",
        "NOTE : Single Ground truth object may assign positive lables to multiple anchors.\n",
        "\n",
        "We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes.\n",
        "\n",
        "\n",
        "Anchors that are neither positive nor negitive do not contribute to the training objective, i.e anchor baoxes\n",
        "with IoU in between 0.3 and 0.7, will be ignored. Ignoring them reduces noise in training and improves stability.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "bbox = np.asarray([[20,30,400,500],[300,400,500,600]], dtype=np.float32)\n",
        "labels = np.asarray([6,8], dtype=np.int8) # 0 is for background, not used here\n",
        "\n",
        "\"\"\"\n",
        "The two labels here represent the two diffent objects, maybe 6 can prepresent a car, and 8 can represent a dog.\n",
        "0 will be reserved for Background.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HH29-qHKzbIz",
        "outputId": "c5f4e6a3-b340-4a33-bbdd-c635b36fe325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "id": "HH29-qHKzbIz",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nThe two labels here represent the two diffent objects, maybe 6 can prepresent a car, and 8 can represent a dog.\\n0 will be reserved for Background.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now we will create a Label array, and initilized it with -1 values. Then calculate IoU at each index\n",
        "of anchor boxes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eNFy-gPizeCo",
        "outputId": "159b261d-7c21-4977-aed1-45fdaf79c10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "id": "eNFy-gPizeCo",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow we will create a Label array, and initilized it with -1 values. Then calculate IoU at each index\\nof anchor boxes.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_inside = np.where(\n",
        "    (anchors[:, 0] >= 0) &   # y1 ≥ 0 (top boundary inside image)\n",
        "    (anchors[:, 1] >= 0) &   # x1 ≥ 0 (left boundary inside image)\n",
        "    (anchors[:, 2] <= 800) & # y2 ≤ 800 (bottom boundary inside image)\n",
        "    (anchors[:, 3] <= 800)   # x2 ≤ 800 (right boundary inside image)\n",
        ")[0]\n",
        "\n",
        "# now this gives index of all valid anchor boxes, that are inside image boundaries."
      ],
      "metadata": {
        "id": "9ur9F4jC9AaE"
      },
      "id": "9ur9F4jC9AaE",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchors[:, 0] # all rows, column 0"
      ],
      "metadata": {
        "id": "8p4zipZ0-5ks",
        "outputId": "84f89205-7698-43a2-d045-074a0d70cdf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8p4zipZ0-5ks",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -37.254834  ,  -82.50966799, -173.01933598, ...,  701.49033201,\n",
              "        610.98066402,  429.96132803])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_inside # is a list of valid anchor box indexes"
      ],
      "metadata": {
        "id": "TH2YkRY9_CAL",
        "outputId": "8f653d52-4f1a-4d84-dc25-82d735895fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TH2YkRY9_CAL",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1410,  1419,  1428, ..., 21075, 21084, 21093])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_inside.shape"
      ],
      "metadata": {
        "id": "4tVSf3LG-8zb",
        "outputId": "592de3c2-a206-4cb5-ce97-09ae5312080b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4tVSf3LG-8zb",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8940,)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = np.empty((len(index_inside),), dtype=np.int32)"
      ],
      "metadata": {
        "id": "kXv0z1S2ALDE"
      },
      "id": "kXv0z1S2ALDE",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label.fill(-1)"
      ],
      "metadata": {
        "id": "8mv33j0nAfXB"
      },
      "id": "8mv33j0nAfXB",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label # initilizing label array with all negative labels"
      ],
      "metadata": {
        "id": "25xqF1FwAhXJ",
        "outputId": "08a9c91a-69e9-4de1-bd10-c85413c6402e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "25xqF1FwAhXJ",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, ..., -1, -1, -1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_anchor_boxes = anchors[index_inside]"
      ],
      "metadata": {
        "id": "Tsfs1WDmAlBe"
      },
      "id": "Tsfs1WDmAlBe",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_anchor_boxes.shape"
      ],
      "metadata": {
        "id": "zwCl1n8bA4J3",
        "outputId": "84ebf118-cf3f-4eeb-fce3-0a8a0737bff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zwCl1n8bA4J3",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8940, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now for each anchor boxes, we need to calculate IoU with each ground truth.\n",
        "\n",
        "Since we have 2 ground truth objects and total of 8940 valid anchor boxes, we should get an\n",
        "array of shape (8940, 2) as output\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ezx2t51GA7Lj",
        "outputId": "f76a64a7-f26a-497f-c66e-1fb5a0abd391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "id": "ezx2t51GA7Lj",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow for each anchor boxes, we need to calculate IoU with each ground truth.\\n\\nSince we have 2 ground truth objects and total of 8940 valid anchor boxes, we should get an\\narray of shape (8940, 2) as output\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ious = np.empty((len(valid_anchor_boxes),2), dtype=np.float32)\n",
        "ious.fill(0)"
      ],
      "metadata": {
        "id": "7NBpLClqBubu"
      },
      "id": "7NBpLClqBubu",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ious"
      ],
      "metadata": {
        "id": "JLF--R2SB6jR",
        "outputId": "d593333e-0e02-448a-eca1-adc730a930fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JLF--R2SB6jR",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       ...,\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for num1, i in enumerate(valid_anchor_boxes):\n",
        "    ya1, xa1, ya2, xa2 = i\n",
        "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
        "    for num2, j in enumerate(bbox):\n",
        "        yb1, xb1, yb2, xb2 = j\n",
        "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
        "\n",
        "        inter_x1 = max([xb1, xa1])\n",
        "        inter_y1 = max([yb1, ya1])\n",
        "        inter_x2 = min([xb2, xa2])\n",
        "        inter_y2 = min([yb2, ya2])\n",
        "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
        "            iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
        "            iou = iter_area / (anchor_area+ box_area - iter_area)\n",
        "        else:\n",
        "            iou = 0.\n",
        "        ious[num1, num2] = iou"
      ],
      "metadata": {
        "id": "y-qlMTCaB-lF"
      },
      "id": "y-qlMTCaB-lF",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ious"
      ],
      "metadata": {
        "id": "A0uebY3SCnwp",
        "outputId": "40ec3fe4-e89b-43e7-a28d-b9e8759da7ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A0uebY3SCnwp",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.06811669, 0.        ],\n",
              "       [0.07083762, 0.        ],\n",
              "       [0.07083762, 0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        ],\n",
              "       [0.        , 0.        ],\n",
              "       [0.        , 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.7\n",
        "matches = np.argwhere(ious > threshold)\n",
        "\n",
        "# Display results\n",
        "for i, j in matches:\n",
        "    print(f\"Anchor {i} and GT Box {j} -> IoU: {ious[i, j]:.4f}\")\n"
      ],
      "metadata": {
        "id": "HFb78b4PCJli"
      },
      "id": "HFb78b4PCJli",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yfky2IbEEIYX"
      },
      "id": "Yfky2IbEEIYX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}